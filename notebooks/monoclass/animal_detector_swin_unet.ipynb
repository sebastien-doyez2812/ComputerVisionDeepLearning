{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cYPZMYrjlay"
      },
      "source": [
        "## **Import libs:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Z0mreftNrMc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.tv_tensors import Image, Mask\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HSoqh2TQJ2zZ"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "BATCHSIZE = 2 # Further reduced batch size to mitigate OutOfMemoryError\n",
        "LR = 1e-5 # Further reduced learning rate for stability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_DVdhVjqTe"
      },
      "source": [
        "## **Check Gpu:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNNT5iJuJxDg",
        "outputId": "72e00830-e2d5-45eb-b3c0-7bfc2afa0898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YeMxafjxEt"
      },
      "source": [
        "## **Architecture:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8nhTpSt69GQ"
      },
      "source": [
        "Pax merging is the equivalent to MaxPool 2x2 is the Unet.\n",
        "I divide the input as an image composed by 4x4 patches.\n",
        "I want to create a image composed by 2x2 patches, so I need to reshape.\n",
        "\n",
        "x.reshape(B, H // 2, 2, W // 2, 2, C) give us (B, H // 2, 2, W // 2, 2, C), but I want B, H//2, W//2, 2C... I need to permute, so I have: B, H//2, W//2, 2, 2, C.\n",
        "After that, I need to reshape to have B, H//2, W//2, 4C, and I need a reducton to have B, H//2, W//2, 2C\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E_ElVy0z69GQ"
      },
      "outputs": [],
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim * 4)\n",
        "        self.reduction = nn.Linear(dim * 4, 2*dim, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, H, W, C = x.shape\n",
        "        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 2, 4, 5).reshape(B, H // 2, W // 2, 4 * C)\n",
        "        x = self.norm(x)\n",
        "        x = self.reduction(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_kDzlju69GR"
      },
      "source": [
        "We also need the equivalent of Upsampling: Patch Expanding..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N1SFGvx-69GR"
      },
      "outputs": [],
      "source": [
        "class PatchExpanding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.expand = nn.Linear(2 * dim, 4 * dim, bias = False)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = B, H /2, W/2, 2C\n",
        "        x = self.expand(x)\n",
        "        # x = B, H/2, W/2, 4C\n",
        "        B, semiH, semiW, Cx4 = x.shape\n",
        "        x = x.view(B, semiH, semiW, 2, 2, Cx4 // 4)\n",
        "        # x = B, H/2, W/2, 2, 2, 4C/4\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5)\n",
        "        # x = B, H/2, 2, W/2, 2, 4C/4\n",
        "        x = x.reshape(B, semiH *2 , semiW * 2,Cx4 // 4)\n",
        "        # x = B, H, W, 4C/4\n",
        "        x = self.norm(x)\n",
        "        return x # B, H, W, C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhlX4ZTs69GR"
      },
      "source": [
        "One of the assets of swin unet, it's this capacity of create tokens from the input, using windows, and doing attention on fixed windows W-MSA vs on swift windows SW-MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TmC939uH69GR"
      },
      "outputs": [],
      "source": [
        "def window_partition(x, window_size):\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    C = windows.shape[-1]\n",
        "    x = windows.view(-1, H //window_size, W // window_size, window_size, window_size, C)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
        "    return x\n",
        "\n",
        "def get_relative_position_index(window_size, device = None):\n",
        "    coords_h = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords_w = torch.arange(window_size, device= device, dtype=torch.long)\n",
        "    coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=\"ij\"))  # 2, Wh, Ww\n",
        "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "    relative_coords[:, :, 0] += window_size - 1  # shift to start from 0\n",
        "    relative_coords[:, :, 1] += window_size - 1\n",
        "    relative_coords[:, :, 0] *= 2 * window_size - 1\n",
        "    relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "    return relative_position_index\n",
        "\n",
        "\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.register_buffer(\"relative_position_index\",  get_relative_position_index(window_size))\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
        "        )\n",
        "\n",
        "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B_, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            N, N, -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "        else:\n",
        "            attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5vn6P4r69GS"
      },
      "source": [
        "Swin Transformer block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qp9sZSUm69GS"
      },
      "outputs": [],
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = WindowAttention(dim, window_size, num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "        if shift_size > 0:\n",
        "            self.register_buffer(\"attention_mask\", self.compute_mask(*input_resolution, device = \"cpu\"))\n",
        "        else:\n",
        "            self.attention_mask = None\n",
        "\n",
        "    def compute_mask(self, H, W, device = None):\n",
        "        img_mask = torch.zeros((1, H, W, 1), device=device)\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(\"-inf\")).masked_fill(attn_mask == 0, float(0.0))\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.attention_mask is not None:\n",
        "            self.attention_mask = self.attention_mask.to(x.device)\n",
        "        B, H, W, C = x.shape\n",
        "        shortcut = x\n",
        "\n",
        "        x = self.norm1(x)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        x_windows = window_partition(x, self.window_size)\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
        "\n",
        "        attn_windows = self.attn(x_windows, mask=self.attention_mask)\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "\n",
        "        x = window_reverse(attn_windows, self.window_size, H, W)\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "\n",
        "        x = shortcut + x\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gKBJ4WP69GS"
      },
      "source": [
        "Swin Transformer (W-MWSA & SW-MSA):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iMzxRRiR69GS"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size=7):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [SwinTransformerBlock(dim=dim,\n",
        "                                  input_resolution=input_resolution,\n",
        "                                  num_heads=num_heads,\n",
        "                                  window_size=window_size,\n",
        "                                  shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                  mlp_ratio=4.,\n",
        "                                  qkv_bias=True,\n",
        "                                  drop=0.,\n",
        "                                  attn_drop=0.,\n",
        "                                  drop_path=0.) for i in range(depth)])\n",
        "        self.downsample = PatchMerging(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        skip_connections = x\n",
        "        x = self.downsample(x)\n",
        "        return x, skip_connections\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size=7):\n",
        "        super().__init__()\n",
        "        self.upsample = PatchExpanding(dim)\n",
        "        self.fusin = nn.Linear(2 * dim, dim)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [SwinTransformerBlock(dim=dim,\n",
        "                                  input_resolution=input_resolution,\n",
        "                                  num_heads=num_heads,\n",
        "                                  window_size=window_size,\n",
        "                                  shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                  mlp_ratio=4.,\n",
        "                                  qkv_bias=True,\n",
        "                                  drop=0.,\n",
        "                                  attn_drop=0.,\n",
        "                                  drop_path=0.) for i in range(depth)])\n",
        "    def forward(self, x, skip_connections):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat((x, skip_connections), -1)\n",
        "        x = self.fusin(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX_I9NdZ69GS",
        "outputId": "27f13061-3933-404d-d1ab-8774b03e6b7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\doyez\\AppData\\Local\\Temp\\ipykernel_43716\\1242657890.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"C:/Users/doyez/OneDrive/Documents/projects/repo_from_github/ComputerVisionDeepLearning/models_saved/monoclass/best_swinunet.pth\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SwinUnet(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 96, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (encoder): ModuleList(\n",
            "    (0): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (1): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (2): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (3): Encoder(\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (downsample): PatchMerging(\n",
            "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
            "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): ModuleList(\n",
            "    (0): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=1536, out_features=3072, bias=False)\n",
            "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=1536, out_features=768, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=768, out_features=1536, bias=False)\n",
            "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=768, out_features=384, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=384, out_features=768, bias=False)\n",
            "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=384, out_features=192, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Decoder(\n",
            "      (upsample): PatchExpanding(\n",
            "        (expand): Linear(in_features=192, out_features=384, bias=False)\n",
            "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fusin): Linear(in_features=192, out_features=96, bias=True)\n",
            "      (blocks): ModuleList(\n",
            "        (0-1): 2 x SwinTransformerBlock(\n",
            "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): WindowAttention(\n",
            "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "          (drop_path): Identity()\n",
            "          (mlp): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bottleneck): Sequential(\n",
            "    (0): SwinTransformerBlock(\n",
            "      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): WindowAttention(\n",
            "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (drop_path): Identity()\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (1): SwinTransformerBlock(\n",
            "      (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): WindowAttention(\n",
            "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "      (drop_path): Identity()\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "        (4): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Conv2d(96, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (final_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=512, patch_size=2, in_chans=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SwinUnet(nn.Module):\n",
        "    def __init__(self, img_size = 256, in_chans=3, num_classes=3, embed_dim=96, depths=[2,2,2,2], patch_size=2, num_heads=[3,6,12,24], window_size=8): # Changed window_size to 8 and num_classes to 3\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size = patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.dim = embed_dim\n",
        "\n",
        "        dim = embed_dim\n",
        "        resolution = img_size // patch_size\n",
        "        for i in range(len(depths)):\n",
        "            self.encoder.append(Encoder(dim=dim, input_resolution=(resolution, resolution), depth=depths[i], num_heads=num_heads[i], window_size=window_size))\n",
        "            dim *= 2\n",
        "            resolution = resolution // 2\n",
        "\n",
        "        # Fix: The bottleneck should use the full 'dim' (1536) from the last encoder, not dim // 2\n",
        "        self.bottleneck = nn.Sequential(*[SwinTransformerBlock(dim=dim, # Changed from dim//2 to dim\n",
        "                                                               input_resolution=(resolution, resolution),\n",
        "                                                               num_heads=num_heads[-1],\n",
        "                                                               window_size=window_size,\n",
        "                                                               shift_size= window_size // 2,\n",
        "                                                               mlp_ratio=4.,\n",
        "                                                               qkv_bias=True,\n",
        "                                                               drop=0.,\n",
        "                                                               attn_drop=0.,\n",
        "                                                               drop_path=0.) for i in range(depths[-1])])\n",
        "        self.decoder = nn.ModuleList()\n",
        "        for i in reversed(range(len(depths))):\n",
        "            dim = dim // 2\n",
        "            resolution = resolution * 2\n",
        "            self.decoder.append(Decoder(dim=dim, input_resolution=(resolution, resolution), depth=depths[i], num_heads=num_heads[i], window_size=window_size))\n",
        "\n",
        "        self.head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
        "        self.final_upsample = nn.Upsample(\n",
        "            scale_factor=patch_size,\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        skip_connections = []\n",
        "        for enc in self.encoder:\n",
        "            x, skip = enc(x)\n",
        "            skip_connections.append(skip)\n",
        "        x = self.bottleneck(x)\n",
        "        for i, dec in enumerate(self.decoder):\n",
        "            skip = skip_connections[-(i+1)]\n",
        "            x = dec(x, skip)\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()\n",
        "        x = self.head(x)\n",
        "        x = self.final_upsample(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = SwinUnet().to(device)\n",
        "\n",
        "# If already trained, load weights\n",
        "model.load_state_dict(torch.load(\"C:/Users/doyez/OneDrive/Documents/projects/repo_from_github/ComputerVisionDeepLearning/models_saved/monoclass/best_swinunet.pth\"))\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wc2sgIXj0Lt"
      },
      "source": [
        "## **DataGenerator:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uram423_j4HU"
      },
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.RandomHorizontalFlip(p = 0.5),\n",
        "    T.RandomRotation(degrees = 15),\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zpdy8rpJkhup"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "\n",
        "\n",
        "class OxfordPetSegmentation(Dataset):\n",
        "    def __init__(self, root, split = \"trainval\", transforms = None):\n",
        "        self.dataset = OxfordIIITPet(root=root, download=True, split = split, target_types = [\"segmentation\", \"category\"])\n",
        "        self.transforms = transforms\n",
        "        self.classes = self.dataset.classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, (mask, label) = self.dataset[idx]\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "        animal_pixels = (mask_np == 1)\n",
        "\n",
        "        multiclass_mask = np.zeros_like(mask_np, dtype=np.uint8)\n",
        "        multiclass_mask[animal_pixels] = 1\n",
        "\n",
        "        image_dp = Image(image)\n",
        "        mask_dp = Mask(multiclass_mask)\n",
        "\n",
        "        # Apply geometric transforms to datapoints\n",
        "        if self.transforms:\n",
        "            # Transforms that work on (dp.Image, dp.Mask) will keep them synchronized\n",
        "            image_dp, mask_dp = self.transforms(image_dp, mask_dp)\n",
        "\n",
        "        image_tensor = T.ToDtype(torch.float32, scale=True)(image_dp)\n",
        "        mask_tensor = torch.as_tensor(mask_dp, dtype=torch.long)\n",
        "\n",
        "        return image_tensor, mask_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IWLutCzzDqkp"
      },
      "outputs": [],
      "source": [
        "train_datset = OxfordPetSegmentation(root = \"data\", split = \"trainval\", transforms = train_transform)\n",
        "val_datset = OxfordPetSegmentation(root = \"data\", split = \"test\", transforms = val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA8i7elGMjJg",
        "outputId": "77670d03-a0f5-490e-e0ae-9c7a398c6d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "torch.Size([256, 256])\n",
            "tensor([0, 1])\n"
          ]
        }
      ],
      "source": [
        "img, mask = train_datset[0]\n",
        "print(img.shape)    # [3, 512, 512]\n",
        "print(mask.shape)   # [512, 512]\n",
        "print(torch.unique(mask))  # tensor([0, 1, 2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On my windows computer, I have to take 0 workers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcyh-zWDGS_3",
        "outputId": "06690f42-6dbc-4b18-e559-c5a0e136eae1"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 0)\n",
        "val_loader = DataLoader(val_datset, batch_size = BATCHSIZE, shuffle = True, num_workers= 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTmK29a5I6G-"
      },
      "source": [
        "## **Training:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G23qjYl5I8bu"
      },
      "outputs": [],
      "source": [
        "# Loss: CrossEntropyLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ks5ToMezJJEo"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Train\", leave = False)\n",
        "    for images, masks in pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix({\"loss\": loss.item()})\n",
        "    return running_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SxmjG6yjGbod"
      },
      "outputs": [],
      "source": [
        "def compute_confusion(preds, targets, class_id):\n",
        "    \"\"\"\n",
        "    preds, targets: [B, H, W] torch tensors\n",
        "    class_id: int (1=cat, 2=dog)\n",
        "    \"\"\"\n",
        "    preds_c = preds == class_id\n",
        "    targets_c = targets == class_id\n",
        "\n",
        "    tp = (preds_c & targets_c).sum().item()\n",
        "    fp = (preds_c & ~targets_c).sum().item()\n",
        "    fn = (~preds_c & targets_c).sum().item()\n",
        "\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "\n",
        "def precision_recall_f1_iou(tp, fp, fn, eps=1e-8):\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "    iou       = tp / (tp + fp + fn + eps)\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efYGnA2ZJjHe"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    stats = {\n",
        "        \"animal\" : {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
        "    }\n",
        "\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        tp, fp, fn = compute_confusion(preds, masks, 1) # Because class are 1 and 2\n",
        "        stats[\"animal\"][\"tp\"] += tp\n",
        "        stats[\"animal\"][\"fp\"] += fp\n",
        "        stats[\"animal\"][\"fn\"] += fn\n",
        "    metrics = {}\n",
        "    tp = stats[\"animal\"][\"tp\"]\n",
        "    fp = stats[\"animal\"][\"fp\"]\n",
        "    fn = stats[\"animal\"][\"fn\"]\n",
        "\n",
        "    precision, recall, f1, iou = precision_recall_f1_iou(tp, fp, fn, eps = 1e-8)\n",
        "    metrics[\"animal\"] = {\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1,\n",
        "    \"iou\": iou,\n",
        "    }\n",
        "    return running_loss / len(dataloader), metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qdOzI_XYJxu4",
        "outputId": "64cea810-23e3-45e0-ba63-c3e16b7713da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                       \r"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'class_idx' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m----> 6\u001b[0m val_loss, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m mean_iou \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      8\u001b[0m     val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manimal\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m ) \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\doyez\\torch-env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[18], line 21\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[1;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     20\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m tp, fp, fn \u001b[38;5;241m=\u001b[39m compute_confusion(preds, masks, \u001b[43mclass_idx\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Because class are 1 and 2\u001b[39;00m\n\u001b[0;32m     22\u001b[0m stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manimal\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tp\n\u001b[0;32m     23\u001b[0m stats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manimal\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m fp\n",
            "\u001b[1;31mNameError\u001b[0m: name 'class_idx' is not defined"
          ]
        }
      ],
      "source": [
        "best_iou = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, device)\n",
        "    mean_iou = (\n",
        "        val_metrics[\"animal\"][\"iou\"]\n",
        "    ) \n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f}\")\n",
        "    print(\n",
        "        f\"Mean IoU: {mean_iou:.3f} | \"\n",
        "        f\"Animal IoU: {val_metrics['animal']['iou']:.3f}\"\n",
        "    )\n",
        "\n",
        "    for cls in [\"animal\"]:\n",
        "        m = val_metrics[cls]\n",
        "        print(\n",
        "            f\"{cls.upper()}  \"\n",
        "            f\"P: {m['precision']:.3f} \"\n",
        "            f\"R: {m['recall']:.3f} \"\n",
        "            f\"F1: {m['f1']:.3f}\"\n",
        "        )\n",
        "\n",
        "    if mean_iou > best_iou:\n",
        "        best_iou = mean_iou\n",
        "        torch.save(model.state_dict(),  \"C:/Users/doyez/OneDrive/Documents/projects/repo_from_github/ComputerVisionDeepLearning/models_saved/monoclass/best_swinunet.pth\")\n",
        "        print(\"Best model saved\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"C:/Users/doyez/OneDrive/Documents/projects/repo_from_github/ComputerVisionDeepLearning/models_saved/monoclass/final_swinunet.pth\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"C:/Users/doyez/OneDrive/Documents/projects/repo_from_github/ComputerVisionDeepLearning/models_saved/monoclass/best_swinunet.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwooMw2KHEh"
      },
      "source": [
        "## **Testing:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roJTeDvNKJ6E"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_binary(pred, target):\n",
        "    pred = pred.bool()\n",
        "    target = target.bool()\n",
        "\n",
        "    tp = (pred & target).sum().item()\n",
        "    fp = (pred & ~target).sum().item()\n",
        "    fn = (~pred & target).sum().item()\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall    = tp / (tp + fn + 1e-8)\n",
        "    f1        = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    iou       = tp / (tp + fp + fn + 1e-8)\n",
        "\n",
        "    return precision, recall, f1, iou\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6avn6yiOoGc"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def testing(model, dataloader, device, num_batches=1):\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {\n",
        "        \"animal\": {\"precision\": [], \"recall\": [], \"f1\": [], \"iou\": []},\n",
        "    }\n",
        "\n",
        "    for batch_idx, (imgs, masks) in enumerate(dataloader):\n",
        "        if batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        imgs = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        imgs = imgs.cpu()\n",
        "        masks = masks.cpu()\n",
        "        preds = preds.cpu()\n",
        "\n",
        "        for i in range(imgs.size(0)):\n",
        "            img = imgs[i].permute(1, 2, 0)\n",
        "            gt = masks[i]\n",
        "            pr = preds[i]\n",
        "\n",
        "            # ===== Metrics =====\n",
        "            # Animal = class 1\n",
        "            p, r, f1, iou = compute_metrics_binary(pr == 1, gt == 1)\n",
        "            metrics[\"animal\"][\"precision\"].append(p)\n",
        "            metrics[\"animal\"][\"recall\"].append(r)\n",
        "            metrics[\"animal\"][\"f1\"].append(f1)\n",
        "            metrics[\"animal\"][\"iou\"].append(iou)\n",
        "\n",
        "            # ===== Plot =====\n",
        "            fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "            fig.suptitle(\n",
        "                f\"Sample {i} | \"\n",
        "                f\"Animal IoU: {metrics['animal']['iou'][-1]:.3f}\",\n",
        "                fontsize=14\n",
        "            )\n",
        "\n",
        "            axs[0, 0].imshow(img)\n",
        "            axs[0, 0].set_title(\"Input Image\")\n",
        "            axs[0, 0].axis(\"off\")\n",
        "\n",
        "            axs[0, 1].imshow(gt == 1, cmap=\"gray\")\n",
        "            axs[0, 1].set_title(\"GT - Animal\")\n",
        "            axs[0, 1].axis(\"off\")\n",
        "\n",
        "            axs[1, 1].imshow(pr == 1, cmap=\"gray\")\n",
        "            axs[1, 1].set_title(\"Pred - Animal\")\n",
        "            axs[1, 1].axis(\"off\")\n",
        "\n",
        "            axs[1, 0].axis(\"off\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    # ===== Aggregate results =====\n",
        "    print(\"\\n===== FINAL METRICS =====\")\n",
        "    for cls in [\"animal\"]:\n",
        "        print(f\"\\nClass: {cls}\")\n",
        "        for m in metrics[cls]:\n",
        "            print(f\"{m}: {sum(metrics[cls][m]) / len(metrics[cls][m]):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "testing(model, val_loader, device, num_batches=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-env-py",
      "language": "python",
      "name": "torch-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
